[{"title":"EMR on EKS Best Practices","type":0,"sectionRef":"#","url":"blog/EMR on EKS Best Practices","content":"","keywords":""},{"title":"EMR Containers Best Practices Guidesâ€‹","type":1,"pageTitle":"EMR on EKS Best Practices","url":"blog/EMR on EKS Best Practices#emr-containers-best-practices-guides","content":"Amazon EMR on Amazon EKS enables you to submit Apache Spark jobs on demand on Amazon Elastic Kubernetes Service (EKS) without provisioning clusters. With EMR on EKS, you can consolidate analytical workloads with your other Kubernetes-based applications on the same Amazon EKS cluster to improve resource utilization and simplify infrastructure management. This link provides the best practices and templates to get started with Amazon EMR on EKS. We publish this guide on GitHub so we could iterate the content quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. Checkout the EMR on EKS Best practices GitHub docs here "},{"title":"Architectureâ€‹","type":1,"pageTitle":"EMR on EKS Best Practices","url":"blog/EMR on EKS Best Practices#architecture","content":"The following diagram illustrates the solution architecture Amazon EMR on EKS.  "},{"title":"Observability EMR on EKS","type":0,"sectionRef":"#","url":"blog/Observability EMR on EKS","content":"","keywords":""},{"title":"Monitoring Amazon EMR on EKS with Amazon Managed Prometheus and Amazon Managed Grafanaâ€‹","type":1,"pageTitle":"Observability EMR on EKS","url":"blog/Observability EMR on EKS#monitoring-amazon-emr-on-eks-with-amazon-managed-prometheus-and-amazon-managed-grafana","content":"In this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases Checkout the full blog here "},{"title":"Architectureâ€‹","type":1,"pageTitle":"Observability EMR on EKS","url":"blog/Observability EMR on EKS#architecture","content":"The following diagram illustrates the solution architecture for scraping Spark Driver and Executorsâ€™ metrics, as well as writing to Amazon Managed Service for Prometheus.  "},{"title":"Grafana Dashboard for Sparkâ€‹","type":1,"pageTitle":"Observability EMR on EKS","url":"blog/Observability EMR on EKS#grafana-dashboard-for-spark","content":"The following Grafana dashboard displays the EMR on EKS Spark job metrics with Driver and Executor details.  "},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"","keywords":""},{"title":"Data on EKS Blogs & Benchmarksâ€‹","type":1,"pageTitle":"Welcome","url":"blog/welcome#data-on-eks-blogs--benchmarks","content":"In this section you will find Blogs and Benchmark reports for the following topics. AWS Data Analytics and ML blogs are featured a short blogs. ðŸš€ EMR on EKS ðŸš€ Spark on EKS ðŸš€ Custom Kubernetes Schedulers (e.g., Apache YuniKorn, Volcano) ðŸš€ Job Schedulers (e.g., Apache Airflow, Argo Workflows) ðŸš€ Distributed Databases (e.g., Cassandra, CockroachDB, MongoDB etc.) ðŸš€ Streaming Platforms (e.g., Apache Kafka, Apache Flink, Apache Beam etc.) "},{"title":"Amazon EMR on Amazon EKS","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks","content":"","keywords":""},{"title":"Benefits of EMR on EKSâ€‹","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#benefits-of-emr-on-eks","content":""},{"title":"Simplify managementâ€‹","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#simplify-management","content":"You get the same EMR benefits for Apache Spark on EKS that you get on EC2 today. This includes fully managed versions of Apache Spark 2.4 and 3.0, automatic provisioning, scaling, performance optimized runtime, and tools like EMR Studiofor authoring jobs and an Apache Spark UI for debugging. "},{"title":"Reduce Costsâ€‹","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#reduce-costs","content":"With EMR on EKS, your compute resources can be shared between your Apache Spark applications and your other Kubernetes applications. Resources are allocated and removed on demand to eliminate over-provisioning or under-utilization of these resources, enabling you to lower costs as you only pay for the resources you use. "},{"title":"Optimize Performanceâ€‹","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#optimize-performance","content":"By running analytics applications on EKS, you can reuse existing EC2 instances in your shared Kubernetes cluster and avoid the startup time of creating a new cluster of EC2 instances dedicated for analytics. You can also get 3x faster performance running performance optimized Spark with EMR on EKS compared to standard Apache Spark on EKS. "},{"title":"EMR on EKS Deployment patterns with Terraformâ€‹","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#emr-on-eks-deployment-patterns-with-terraform","content":"The following Terraform templates are available to deploy. Monitoring EMR on EKS Spark jobs with Prometheus Server, Amazon Managed Prometheus and Amazon Managed GrafanaRunning EMR on EKS Spark Jobs with FSx for Lustre as Shuffle StorageScaling EMR on EKS Spark Jobs with Karpenter Autoscaler "},{"title":"AI/ML Platforms on EKS","type":0,"sectionRef":"#","url":"docs/ai-ml-eks","content":"AI/ML Platforms on EKS info Work is still in progress with the documentation.","keywords":""},{"title":"Monitoring EMR on EKS","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg","content":"","keywords":""},{"title":"Introductionâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#introduction","content":"In this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases. "},{"title":"Deploying the Solutionâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS, as well as monitor spark job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only)Two managed node groups Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc.Spark Node group with single AZ for running Spark jobs Enable EMR on EKS and creates two Data teams (emr-data-team-a, emr-data-team-b) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster for emr-data-team-aIAM policy for emr-data-team-aAmazon Managed Prometheus workspace to remote write metrics from Prometheus serverDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, VPA for Prometheus, AWS for FluentBit, CloudWatchMetrics for EKS "},{"title":"Prerequisites:â€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deployâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd analytics/emr-eks-amp-amg terraform init  Set AWS_REGION and Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;&lt;enter-your-region&gt;&quot; terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Verify the resourcesâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#verify-the-resources","content":"Letâ€™s verify the resources created by terraform apply. Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus. aws eks describe-cluster --name emr-eks-amp-amg aws amp list-workspaces --alias amp-ws-emr-eks-amp-amg  Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region &lt;ENTER_YOUR_REGION&gt; update-kubeconfig --name emr-eks-amp-amg # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Setup Amazon Managed Grafana with SSOâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#setup-amazon-managed-grafana-with-sso","content":"Currently, this step is manual. Please follow the steps in this blog to create Amazon Managed Grafana with SSO enabled in your account. You can visualize the Spark jobs runs and metrics using Amazon Managed Prometheus and Amazon Managed Grafana. "},{"title":"Execute Sample Spark job on EMR Virtual Clusterâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#execute-sample-spark-job-on-emr-virtual-cluster","content":"Execute the Spark job using the below shell script. This script requires three input parameters in which EMR_VIRTUAL_CLUSTER_ID and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_ID=$1 # Terraform output variable is emrcontainers_virtual_cluster_id S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd analytics/emr-eks-amp-amg/examples/spark/ ./emr-eks-spark-amp-amg.sh &quot;&lt;ENTER_EMR_VIRTUAL_CLUSTER_ID&gt;&quot; &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Verify the job execution kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Cleanupâ€‹","type":1,"pageTitle":"Monitoring EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-amp-amg#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"EMR on EKS with FSx for Lustre","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre","content":"","keywords":""},{"title":"Introductionâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#introduction","content":"Amazon FSx for Lustre is a fully managed shared storage option built on the worldâ€™s most popular high-performance file system. It offers highly scalable, cost-effective storage, which provides sub-millisecond latencies, millions of IOPS, and throughput of hundreds of gigabytes per second. Its popular use cases include high-performance computing (HPC), financial modeling, video rendering, and machine learning. FSx for Lustre supports two types of deployments: For storage, EMR on EKS supports node ephemeral storage using hostPath where the storage is attached to individual nodes, and Amazon Elastic Block Store (Amazon EBS) volume per executor/driver pod using dynamic Persistent Volume Claims. However, some Spark users are looking for an HDFS-like shared file system to handle specific workloads like time-sensitive applications or streaming analytics. In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage for running Spark jobs with EMR on EKS. "},{"title":"Deploying the Solutionâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS with FSx for Lustre as shuffle storage, as well as monitor spark job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with two managed node groupsDeploys Metrics server with HA, Cluster Autoscaler, Prometheus, VPA, CoreDNS Autoscaler, FSx CSI driverEMR on EKS Teams and EMR Virtual cluster for emr-data-team-aCreates Amazon managed Prometheus Endpoint and configures Prometheus Server addon with remote write configuration to Amazon Managed PrometheusCreates PERSISTENT type FSx for Lustre filesystem, Static Persistent volume and Persistent volume claimCreates Scratch type FSx for Lustre filesystem with dynamic Persistent volume claimS3 bucket to sync FSx for Lustre filesystem data "},{"title":"Prerequisites:â€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deployâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd analytics/emr-eks-fsx-lustre terraform init  Set AWS_REGION and Runterraform plan to verify the resources created by this execution. export AWS_REGION=&quot;&lt;enter-your-region&gt;&quot; terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Verify the resourcesâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#verify-the-resources","content":"Letâ€™s verify the resources created by terraform apply. Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus aws eks describe-cluster --name emr-eks-fsx-lustre aws amp list-workspaces --alias amp-ws-emr-eks-fsx-lustre  # Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region &lt;ENTER_YOUR_REGION&gt; update-kubeconfig --name emr-eks-fsx-lustre # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod kubectl get pods -n kube-system | grep fsx # Output of the FSx controller and node pods kubectl get pvc -n emr-data-team-a # Output of persistent volume for static(`fsx-static-pvc`) and dynamic(`fsx-dynamic-pvc`) #FSx Storage Class kubectl get storageclasses | grep fsx emr-eks-fsx-lustre fsx.csi.aws.com Delete Immediate false 109s # Output of static persistent volume with name `fsx-static-pv` kubectl get pv | grep fsx fsx-static-pv 1000Gi RWX Recycle Bound emr-data-team-a/fsx-static-pvc fsx # Output of static persistent volume with name `fsx-static-pvc` and `fsx-dynamic-pvc` # Pending status means that the FSx for Lustre is still getting created. This will be changed to bound once the filesystem is created. Login to AWS console to verify. kubectl get pvc -n emr-data-team-a | grep fsx fsx-dynamic-pvc Pending fsx 4m56s fsx-static-pvc Bound fsx-static-pv 1000Gi RWX fsx 4m56s  "},{"title":"Spark Job Execution - FSx for Lustre Static Provisioningâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#spark-job-execution---fsx-for-lustre-static-provisioning","content":"Execute Spark Job by using FSx for Lustre with statically provisioned volume. Execute the Spark job using the below shell script. This script requires three input parameters which can be extracted from terraform apply output values. EMR_VIRTUAL_CLUSTER_ID=$1 # Terraform output variable is emrcontainers_virtual_cluster_id S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd analytics/emr-eks-fsx-lustre/examples/spark-execute/ ./fsx-static-spark.sh &quot;&lt;ENTER_EMR_VIRTUAL_CLUSTER_ID&gt;&quot; &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Verify the job execution events kubectl get pods --namespace=emr-data-team-a -w  This will show the mounted /data directory with FSx DNS name kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /static  "},{"title":"Spark Job Execution - FSx for Lustre Dynamic Provisioningâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#spark-job-execution---fsx-for-lustre-dynamic-provisioning","content":"Execute Spark Job by using FSx for Lustre with dynamically provisioned volume and Fsx for Lustre file system This script requires three input parameters in which EMR_VIRTUAL_CLUSTER_ID and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_ID=$1 # Terraform output variable is emrcontainers_virtual_cluster_id S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd analytics/emr-eks-fsx-lustre/examples/spark-execute/ ./fsx-dynamic-spark.sh &quot;&lt;ENTER_EMR_VIRTUAL_CLUSTER_ID&gt;&quot; &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Verify the job execution events kubectl get pods --namespace=emr-data-team-a -w  kubectl exec -ti ny-taxi-trip-dyanmic-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h kubectl exec -ti ny-taxi-trip-dyanmic-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /dyanmic  "},{"title":"Cleanupâ€‹","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"EMR on EKS with Karpenter Autoscaler","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-karpenter","content":"","keywords":""},{"title":"Introductionâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#introduction","content":"In this pattern, you will learn how to deploy, configure and use multiple Karpenter provisioners for scaling Spark jobs with EMR on EKS. Multiple Data teams within the organization can run Spark jobs on the selected Karpenter provisioners using tolerations specified in the pod templates example. This pattern deploys three Karpenter provisioners. spark-compute-optimized provisioner to run spark jobs on c5d instances.spark-memory-optimized provisioner to run spark jobs on r5d instances.spark-graviton-memory-optimized provisioner to run spark jobs on r6gd Graviton instances(ARM64). Let's review the Karpenter provisioner for computed optimized instances deployed by this pattern. Karpenter provisioner for compute optimized instances. This template leverages the pre-created AWS Launch templates. apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: spark-compute-optimized namespace: karpenter # Same namespace as Karpenter add-on installed spec: kubeletConfiguration: containerRuntime: containerd # podsPerCore: 2 # maxPods: 20 requirements: - key: &quot;topology.kubernetes.io/zone&quot; operator: In values: [${azs}a] #Update the correct region and zones - key: &quot;karpenter.sh/capacity-type&quot; operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] - key: &quot;node.kubernetes.io/instance-type&quot; #If not included, all instance types are considered operator: In values: [&quot;c5d.large&quot;,&quot;c5d.xlarge&quot;,&quot;c5d.2xlarge&quot;,&quot;c5d.4xlarge&quot;,&quot;c5d.9xlarge&quot;] # 1 NVMe disk - key: &quot;kubernetes.io/arch&quot; operator: In values: [&quot;amd64&quot;] limits: resources: cpu: 1000 providerRef: name: spark-compute-optimized labels: type: karpenter provisioner: spark-compute-optimized NodeGroupType: SparkComputeOptimized taints: - key: spark-compute-optimized value: 'true' effect: NoSchedule ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: spark-compute-optimized namespace: karpenter spec: subnetSelector: Name: &quot;${eks_cluster_id}-private*&quot; # required launchTemplate: &quot;${launch_template_name}&quot; # optional, see Launch Template documentation tags: InstanceType: &quot;spark-compute-optimized&quot; # optional, add tags for your own use  Spark Jobs can use this provisioner to submit the jobs by adding tolerations to pod templates. e.g., spec: tolerations: - key: &quot;spark-compute-optimized&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;  Karpenter provisioner for memory optimized instances. This template uses the AWS Node template with Userdata. apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: spark-memory-optimized namespace: karpenter spec: kubeletConfiguration: containerRuntime: containerd # podsPerCore: 2 # maxPods: 20 requirements: - key: &quot;topology.kubernetes.io/zone&quot; operator: In values: [${azs}b] #Update the correct region and zone - key: &quot;karpenter.sh/capacity-type&quot; operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] - key: &quot;node.kubernetes.io/instance-type&quot; #If not included, all instance types are considered operator: In values: [&quot;r5d.4xlarge&quot;,&quot;r5d.8xlarge&quot;,&quot;r5d.8xlarge&quot;] # 2 NVMe disk - key: &quot;kubernetes.io/arch&quot; operator: In values: [&quot;amd64&quot;] limits: resources: cpu: 1000 providerRef: # optional, recommended to use instead of `provider` name: spark-memory-optimized labels: type: karpenter provisioner: spark-memory-optimized NodeGroupType: SparkMemoryOptimized taints: - key: spark-memory-optimized value: 'true' effect: NoSchedule ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: spark-memory-optimized namespace: karpenter spec: blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 200Gi volumeType: gp3 encrypted: true deleteOnTermination: true metadataOptions: httpEndpoint: enabled httpProtocolIPv6: disabled httpPutResponseHopLimit: 2 httpTokens: required subnetSelector: Name: &quot;${eks_cluster_id}-private*&quot; # Name of the Subnets to spin up the nodes securityGroupSelector: # required, when not using launchTemplate Name: &quot;${eks_cluster_id}-node*&quot; # name of the SecurityGroup to be used with Nodes instanceProfile: &quot;${instance_profile}&quot; # optional, if already set in controller args userData: | MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=&quot;BOUNDARY&quot; --BOUNDARY Content-Type: text/x-shellscript; charset=&quot;us-ascii&quot; #!/bin/bash echo &quot;Running a custom user data script&quot; set -ex IDX=1 DEVICES=$(lsblk -o NAME,TYPE -dsn | awk '/disk/ {print $1}') for DEV in $DEVICES do mkfs.xfs /dev/$${DEV} mkdir -p /local$${IDX} echo /dev/$${DEV} /local$${IDX} xfs defaults,noatime 1 2 &gt;&gt; /etc/fstab IDX=$(($${IDX} + 1)) done mount -a /usr/bin/chown -hR +999:+1000 /local* --BOUNDARY-- tags: InstanceType: &quot;spark-memory-optimized&quot; # optional, add tags for your own use  Spark Jobs can use this provisioner to submit the jobs by adding tolerations to pod templates. e.g., spec: tolerations: - key: &quot;spark-memory-optimized&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;  "},{"title":"Deploying the Solutionâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS with Karpenter as Autoscaler, as well as monitor job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only)One managed node group Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc. Enables EMR on EKS and creates two Data teams (emr-data-team-a, emr-data-team-b) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster for emr-data-team-a and IAM policy for emr-data-team-aAmazon Managed Prometheus workspace to remotely write metrics from Prometheus serverDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Karpetner, Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, VPA for Prometheus, AWS for FluentBit, CloudWatchMetrics for EKS "},{"title":"Prerequisites:â€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deployâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd analytics/emr-eks-karpenter terraform init  Set AWS_REGION and Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;&lt;enter-your-region&gt;&quot; terraform plan  This command may take between 20 and 30 minutes to create all the resources. terraform apply  Enter yes to apply. "},{"title":"Verify the resourcesâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#verify-the-resources","content":"Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus aws eks describe-cluster --name emr-eks-karpenter aws amp list-workspaces --alias amp-ws-emr-eks-karpenter  Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region &lt;ENTER_YOUR_REGION&gt; update-kubeconfig --name emr-eks-karpenter # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Execute Sample Spark jobâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#execute-sample-spark-job","content":""},{"title":"Execute the sample PySpark Job to trigger compute optimized Karpenter provisionerâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#execute-the-sample-pyspark-job-to-trigger-compute-optimized-karpenter-provisioner","content":"The following script requires three input parameters in which EMR_VIRTUAL_CLUSTER_NAME and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_NAME=$1 # Terraform output variable is emrcontainers_virtual_cluster_name S3_BUCKET=$2 # This script requires S3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd analytics/emr-eks-karpenter/examples/spark/ ./compute-nytaxi-pyspark-karpenter.sh &quot;&lt;EMR_VIRTUAL_CLUSTER_NAME&gt;&quot; \\ &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; \\ &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job executionâ€‹ kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Execute the sample PySpark Job to trigger Memory optimized Karpenter provisionerâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#execute-the-sample-pyspark-job-to-trigger-memory-optimized-karpenter-provisioner","content":"This pattern uses the Karpenter provisioner for memory optimized instances. This template leverages the Karpenter AWS Node template with Userdata. cd analytics/emr-eks-karpenter/examples/spark/ ./memory-nytaxi-pyspark-karpenter.sh &quot;&lt;EMR_VIRTUAL_CLUSTER_NAME&gt;&quot; \\ &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; \\ &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job executionâ€‹ kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Cleanupâ€‹","type":1,"pageTitle":"EMR on EKS with Karpenter Autoscaler","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Introduction","type":0,"sectionRef":"#","url":"docs/intro","content":"Introduction Data on Amazon EKS (DoEKS) is an open source repository to build, deploy and scale Data workloads on Amazon EKS. This repo provides the Terraform templates to build scalable data platform on Amazon EKS for the following categories. Features ðŸš€ EMR on EKS ðŸš€ Open Source Spark on EKS ðŸš€ Custom Kubernetes Schedulers (e.g., Apache YuniKorn, Volcano) ðŸš€ Job Schedulers (e.g., Apache Airflow, Argo Workflows) ðŸš€ AI/ML on Kubernetes (e.g., KubeFlow, MLFlow, Tensorflow, PyTorch etc.) ðŸš€ Distributed Databases (e.g., Cassandra, CockroachDB, MongoDB etc.) ðŸš€ Streaming Platforms (e.g., Apache Kafka, Apache Flink, Apache Beam etc.) Getting Started Checkout the documentation for each section to deploy infrastructure and run sample Spark/ML jobs.","keywords":""},{"title":"Job schedulers on EKS","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks","content":"Job schedulers on EKS info Work is still in progress with the documentation.","keywords":""},{"title":"Distributed Databases on EKS","type":0,"sectionRef":"#","url":"docs/distributed-databases-eks","content":"Distributed Databases on EKS info Work is still in progress with the documentation.","keywords":""},{"title":"Amazon Manged Workflows for Apache Airflow (MWAA)","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks/aws-managed-airflow","content":"","keywords":""},{"title":"Considerationsâ€‹","type":1,"pageTitle":"Amazon Manged Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#considerations","content":"Ideally we recommend adding the steps to sync requirements/sync dags to the MWAA S3 Bucket as part of a CI/CD pipeline. Generally Dags development have a different lifecycle than the Terraform code to provision infrastructure. For simplicity, we are providing steps for that using Terraform running AWS CLI commands on null_resource. "},{"title":"Prerequisites:â€‹","type":1,"pageTitle":"Amazon Manged Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#prerequisites","content":"Ensure that you have the following tools installed locally: aws clikubectlterraform "},{"title":"Deployâ€‹","type":1,"pageTitle":"Amazon Manged Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#deploy","content":"To provision this example: terraform init terraform apply  Enter yes at command prompt to apply Once done, you will see terraform output like below.  The following components are provisioned in your environment: A sample VPC, 3 Private Subnets and 3 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S metrics server and cluster autoscalerA MWAA environment in version 2.2.2An EMR virtual cluster registered with the newly created EKSA S3 bucket with DAG code "},{"title":"Validateâ€‹","type":1,"pageTitle":"Amazon Manged Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#validate","content":"The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment. Run update-kubeconfig command: aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt;  List the nodes running currently kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-0-0-42.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326 ip-10-0-22-71.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326 ip-10-0-44-63.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326  List the namespaces in your EKS cluster kubectl get ns # Output should look like below default Active 4h38m emr-mwaa Active 4h34m kube-node-lease Active 4h39m kube-public Active 4h39m kube-system Active 4h39m mwaa Active 4h30m  namesapce emr-mwaa will be used by EMR for running spark jobs. namesapce mwaa will be used by MWAA directly. Trigger jobs from MWAA Log into Apache Airflow UI Open the Environments page on the Amazon MWAA consoleChoose an environmentUnder the Details section, click the link for the Airflow UI Trigger the DAG workflow to execute job in EKS In the Airflow UI, enable the example DAG kubernetes_pod_example and then trigger it.   Verify that the pod was executed successfully After it runs and completes successfully, use the following command to verify the pod: kubectl get pods -n mwaa  You should see output similar to the following: NAME READY STATUS RESTARTS AGE mwaa-pod-test.4bed823d645844bc8e6899fd858f119d 0/1 Completed 0 25s  Trigger the DAG workflow to execute job in EMR on EKS First, you need to set up the connection to EMR virtual cluster in MWAA Click Add button, make sure use emr_eks as Connection Id Amazon Web Services as Connection Type replace the value in Extra based on your terraform output  Go back to Airflow UI main page, enable the example DAG emr_eks_pi_job and then trigger it. While it is running, use the following command to verify the spark jobs: kubectl get all -n emr-mwaa  You should see output similar to the following: NAME READY STATUS RESTARTS AGE pod/000000030tk2ihdmr8g-psstj 3/3 Running 0 90s pod/pythonpi-a8051f83b415c911-exec-1 2/2 Running 0 14s pod/pythonpi-a8051f83b415c911-exec-2 2/2 Running 0 14s pod/spark-000000030tk2ihdmr8g-driver 2/2 Running 0 56s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/spark-000000030tk2ihdmr8g-ee64be83b4151dd5-driver-svc ClusterIP None &lt;none&gt; 7078/TCP,7079/TCP,4040/TCP 57s NAME COMPLETIONS DURATION AGE job.batch/000000030tk2ihdmr8g 0/1 92s 92s  You can also check the job status in Amazon EMR console. Under the Virtual clusters section, click the your Virtual cluster "},{"title":"Destroyâ€‹","type":1,"pageTitle":"Amazon Manged Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#destroy","content":"To teardown and remove the resources created in this example: terraform destroy -auto-approve   "},{"title":"Apache Spark on EKS","type":0,"sectionRef":"#","url":"docs/spark-on-eks","content":"Apache Spark on EKS info Work is still in progress with the documentation.","keywords":""},{"title":"Running Spark jobs with Spark Operator and YuniKorn","type":0,"sectionRef":"#","url":"docs/spark-on-eks/spark-operator-yunikorn","content":"","keywords":""},{"title":"Introductionâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#introduction","content":"In this post, we will learn to build, configure and deploy highly scalable EKS Cluster with Open source Spark Operator and Apache YuniKorn batch scheduler. "},{"title":"Architectureâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#architecture","content":"Spark Operator consists of... a SparkApplication controller that watches events of creation, updates, and deletion of SparkApplication objects and acts on the watch events,a submission runner that runs spark-submit for submissions received from the controller,a Spark pod monitor that watches for Spark pods and sends pod status updates to the controller,a Mutating Admission Webhook that handles customizations for Spark driver and executor pods based on the annotations on the pods added by the controller,and also a command-line tool named sparkctl for working with the operator. The following diagram shows how different components of Spark Operator add-pn interact and work together.  "},{"title":"Deploying the Solutionâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs with open source Spark Operator and Apache YuniKorn. This example deploys an EKS Cluster running the Spark K8s Operator into a new VPC. Creates a new sample VPC, 3 Private Subnets and 3 Public SubnetsCreates Internet gateway for Public Subnets and NAT Gateway for Private SubnetsCreates EKS Cluster Control plane with public endpoint (for demo reasons only) with one managed node groupDeploys Metrics server, Cluster Autoscaler, Spark-k8s-operator, Apache Yunikorn and Prometheus server.Spark Operator is a Kubernetes Operator for Apache Spark deployed to spark-operator namespace. The operator by default watches and handles SparkApplications in all namespaces. "},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deployâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd analytics/spark-k8s-operator terraform init  Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&lt;enter-your-region&gt; # Select your own region terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Execute Sample Spark Job on EKS Cluster with spark-k8s-operatorâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#execute-sample-spark-job-on-eks-cluster-with-spark-k8s-operator","content":" cd analytics/spark-k8s-operator/spark-samples kubectl apply -f pyspark-pi-job.yaml  "},{"title":"Verify the Spark job statusâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#verify-the-spark-job-status","content":" kubectl get sparkapplications -n spark-team-a kubectl describe sparkapplication pyspark-pi -n spark-team-a  "},{"title":"Cleanupâ€‹","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Self-managed Apache Airflow deployment for EKS","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks/self-managed-airflow","content":"","keywords":""},{"title":"Prerequisites:â€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deployâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd schedulers/self-managed-airflow terraform init  Set AWS_REGION and Run terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;&lt;enter-your-region&gt;&quot; terraform plan  Deploy the pattern terraform apply  Enter yes to apply. info Rerun terraform apply if your execution timed out. "},{"title":"Verify the resourcesâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-resources","content":""},{"title":"Create kubectl configâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#create-kubectl-config","content":"aws eks --region &quot;&lt;ENTER_REGION&gt;&quot; update-kubeconfig --name &quot;&lt;ENTER_EKS_CLUSTER_ID&gt;&quot;  "},{"title":"Describe the EKS Clusterâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#describe-the-eks-cluster","content":"aws eks describe-cluster --name self-managed-airflow  "},{"title":"Verify the EFS PV and PVC created by this deploymentâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-efs-pv-and-pvc-created-by-this-deployment","content":"kubectl get pvc -n airflow NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE airflow-dags Bound pvc-157cc724-06d7-4171-a14d-something 10Gi RWX efs-sc 73m kubectl get pv -n airflow NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-157cc724-06d7-4171-a14d-something 10Gi RWX Delete Bound airflow/airflow-dags efs-sc 74m  "},{"title":"Verify the EFS Filesystemâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-efs-filesystem","content":"aws efs describe-file-systems --query &quot;FileSystems[*].FileSystemId&quot; --output text  "},{"title":"Verify S3 bucket created for Airflow logsâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-s3-bucket-created-for-airflow-logs","content":"aws s3 ls | grep airflow-logs-  "},{"title":"Verify the Airflow deploymentâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-airflow-deployment","content":"kubectl get deployment -n airflow NAME READY UP-TO-DATE AVAILABLE AGE airflow-pgbouncer 1/1 1 1 77m airflow-scheduler 2/2 2 2 77m airflow-statsd 1/1 1 1 77m airflow-triggerer 1/1 1 1 77m airflow-webserver 2/2 2 2 77m  "},{"title":"Fetch Postgres RDS passwordâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#fetch-postgres-rds-password","content":"Amazon Postgres RDS database password can be fetched from the Secrets manager Login to AWS console and open secrets managerClick on postgres secret nameClick on Retrieve secret value button to verify the Postgres DB master password "},{"title":"Login to Airflow Web UIâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#login-to-airflow-web-ui","content":"This deployment creates an Ingress object with public LoadBalancer(internet-facing) for demo purpose For production workloads, you can modify values.yaml to choose internal LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port. Execute the following command to get the ALB DNS name kubectl get ingress -n airflow NAME CLASS HOSTS ADDRESS PORTS AGE airflow-airflow-ingress alb * k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com 80 88m  The above ALB URL will be different for you deployment. So use your URL and open it in a brower e.g., Open URL http://k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com/ in a browser By default, Airflow creates a default user with admin and password as admin Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user "},{"title":"Create S3 Connection from Airflow Web UIâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#create-s3-connection-from-airflow-web-ui","content":"This step is critical for writing the Airflow logs to S3 bucket. Login to Airflow WebUI with admin and password as admin using ALB URLSelect Admin dropdown and Click on ConnectionsClick on &quot;+&quot; button to add a new recordEnter Connection Id as aws_s3_conn, Connection Type as Amazon Web Services and Extra as {&quot;region_name&quot;: &quot;&lt;ENTER_YOUR_REGION&gt;&quot;}Click on Save button  "},{"title":"Execute Sample Airflow Jobâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#execute-sample-airflow-job","content":"Login to Airflow WebUIClick on DAGs link on the top of the page. This will show two dags pre-created by the GitSync featureExecute the first DAG by clicking on Play button (&gt;)Verify the DAG execution from Graph linkAll the Tasks will go green after few minutesClick on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3 "},{"title":"Cleanupâ€‹","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.db&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  Make sure all the S3 buckets are empty and deleted once your test is finished  "},{"title":"Streaming Platforms on EKS","type":0,"sectionRef":"#","url":"docs/streaming-platforms-eks","content":"Streaming Platforms on EKS info Please note that Streaming platform work is currently in progress. Docs will be updated once the deployment example is added to this repo. The following streaming platforms can be deployed and scale on Amazon EKS. KafkaFlinkTrino info Presto SQL is now Trino","keywords":""}]