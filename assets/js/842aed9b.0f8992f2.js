"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3312],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>c});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=p(n),c=r,k=d["".concat(l,".").concat(c)]||d[c]||m[c]||o;return n?a.createElement(k,s(s({ref:t},u),{},{components:n})):a.createElement(k,s({ref:t},u))}));function c(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,s=new Array(o);s[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,s[1]=i;for(var p=2;p<o;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},769:(e,t,n)=>{n.d(t,{Z:()=>c});var a=n(7294),r=n(5697),o=n.n(r),s=n(6010);const i="collapsibleContent_q3kw",l="header_QCEw",p="icon_PckA",u="content_qLC1",m="expanded_iGsi";function d(e){let{children:t,header:n}=e;const[r,o]=(0,a.useState)(!1);return a.createElement("div",{className:i},a.createElement("div",{className:(0,s.Z)(l,{[m]:r}),onClick:()=>{o(!r)}},n,a.createElement("span",{className:(0,s.Z)(p,{[m]:r})})),r&&a.createElement("div",{className:u},t))}d.propTypes={children:o().node.isRequired,header:o().node.isRequired};const c=d},8391:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>d,frontMatter:()=>s,metadata:()=>l,toc:()=>u});var a=n(7462),r=(n(7294),n(3905)),o=n(769);const s={sidebar_position:2,sidebar_label:"EMR on EKS with Karpenter"},i="EMR on EKS with [Karpenter](https://karpenter.sh/)",l={unversionedId:"amazon-emr-on-eks/emr-eks-karpenter",id:"amazon-emr-on-eks/emr-eks-karpenter",title:"EMR on EKS with [Karpenter](https://karpenter.sh/)",description:"Introduction",source:"@site/docs/amazon-emr-on-eks/emr-eks-karpenter.md",sourceDirName:"amazon-emr-on-eks",slug:"/amazon-emr-on-eks/emr-eks-karpenter",permalink:"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/amazon-emr-on-eks/emr-eks-karpenter.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,sidebar_label:"EMR on EKS with Karpenter"},sidebar:"docs",previous:{title:"Introduction",permalink:"/data-on-eks/docs/amazon-emr-on-eks/"},next:{title:"EMR on EKS with Fargate",permalink:"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-fargate"}},p={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites:",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"Execute the sample PySpark Job to trigger compute optimized Karpenter provisioner",id:"execute-the-sample-pyspark-job-to-trigger-compute-optimized-karpenter-provisioner",level:3},{value:"Verify the job execution",id:"verify-the-job-execution",level:4},{value:"Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter provisioner",id:"execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-provisioner",level:3},{value:"Execute the sample PySpark Job to trigger Memory optimized Karpenter provisioner",id:"execute-the-sample-pyspark-job-to-trigger-memory-optimized-karpenter-provisioner",level:3},{value:"Verify the job execution",id:"verify-the-job-execution-1",level:4},{value:"Execute the sample PySpark Job to trigger Graviton Memory optimized Karpenter provisioner",id:"execute-the-sample-pyspark-job-to-trigger-graviton-memory-optimized-karpenter-provisioner",level:3},{value:"Verify the job execution",id:"verify-the-job-execution-2",level:4},{value:"FSx for Lustre",id:"fsx-for-lustre",level:2},{value:"Apache YuniKorn - Batch Scheduler",id:"apache-yunikorn---batch-scheduler",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Verify the job execution",id:"verify-the-job-execution-3",level:4}],m={toc:u};function d(e){let{components:t,...s}=e;return(0,r.kt)("wrapper",(0,a.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"emr-on-eks-with-karpenter"},"EMR on EKS with ",(0,r.kt)("a",{parentName:"h1",href:"https://karpenter.sh/"},"Karpenter")),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"In this ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/emr-eks-karpenter"},"pattern"),", you will deploy an EMR on EKS cluster and use Karpenter provisioners for scaling Spark jobs.\nIt will demonstrate how to use multiple storage types (",(0,r.kt)("strong",{parentName:"p"},"EBS PVC, Instance Storage (SSD), FSx for Lustre"),") for ",(0,r.kt)("strong",{parentName:"p"},"Spark shuffle storage"),"."),(0,r.kt)("p",null,"Additionally, all the ",(0,r.kt)("strong",{parentName:"p"},"Karpenter Node templates")," use ",(0,r.kt)("strong",{parentName:"p"},"RAID0 configuration")," to ensure Spark jobs can refer to ",(0,r.kt)("inlineCode",{parentName:"p"},"/local1")," as one folder even if the instances have one or more SSD disks."),(0,r.kt)("p",null,"This pattern deploys three Karpenter provisioners, and it also provides guidance on using ",(0,r.kt)("strong",{parentName:"p"},"Apache YuniKorn as a batch scheduler")," to ",(0,r.kt)("strong",{parentName:"p"},"gang-schedule")," Spark jobs."),(0,r.kt)("p",null,"This example showcases how multiple data teams within an organization can run Spark jobs using Karpenter provisioners that are unique to each workload.\nFor example, you can use a compute-optimized provisioner that has taints and use pod templates to specify tolerations so that you can run Spark on compute-optimized EC2 instances."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"spark-compute-optimized")," provisioner to run spark jobs on ",(0,r.kt)("inlineCode",{parentName:"li"},"c5d")," instances."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"spark-memory-optimized")," provisioner to run spark jobs on ",(0,r.kt)("inlineCode",{parentName:"li"},"r5d")," instances."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"spark-graviton-memory-optimized")," provisioner to run spark jobs on ",(0,r.kt)("inlineCode",{parentName:"li"},"r6gd")," Graviton instances(",(0,r.kt)("inlineCode",{parentName:"li"},"ARM64"),").")),(0,r.kt)("p",null,"Let's review the Karpenter provisioner for computed optimized instances deployed by this pattern."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Karpenter provisioner for compute optimized instances. This template leverages the pre-created AWS Launch templates.")),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Karpenter Provisioner - Compute Optimized Instances")),mdxType:"CollapsibleContent"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: spark-compute-optimized\n  namespace: karpenter # Same namespace as Karpenter add-on installed\nspec:\n  kubeletConfiguration:\n    containerRuntime: containerd\n    #    podsPerCore: 2\n    #    maxPods: 20\n  requirements:\n    - key: "topology.kubernetes.io/zone"\n      operator: In\n      values: [${azs}a] #Update the correct region and zones\n    - key: "karpenter.sh/capacity-type"\n      operator: In\n      values: ["spot", "on-demand"]\n    - key: "node.kubernetes.io/instance-type" #If not included, all instance types are considered\n      operator: In\n      values: ["c5d.large","c5d.xlarge","c5d.2xlarge","c5d.4xlarge","c5d.9xlarge"] # 1 NVMe disk\n    - key: "kubernetes.io/arch"\n      operator: In\n      values: ["amd64"]\n  limits:\n    resources:\n      cpu: 1000\n  providerRef:\n    name: spark-compute-optimized\n  labels:\n    type: karpenter\n    provisioner: spark-compute-optimized\n    NodeGroupType: SparkComputeOptimized\n  taints:\n    - key: spark-compute-optimized\n      value: \'true\'\n      effect: NoSchedule\n  ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set\n\n---\napiVersion: karpenter.k8s.aws/v1alpha1\nkind: AWSNodeTemplate\nmetadata:\n  name: spark-compute-optimized\n  namespace: karpenter\nspec:\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 100Gi\n        volumeType: gp3\n        encrypted: true\n        deleteOnTermination: true\n  metadataOptions:\n    httpEndpoint: enabled\n    httpProtocolIPv6: disabled\n    httpPutResponseHopLimit: 2\n    httpTokens: required\n  subnetSelector:\n    Name: "${eks_cluster_id}-private*"        # Name of the Subnets to spin up the nodes\n  securityGroupSelector:                      # required, when not using launchTemplate\n    Name: "${eks_cluster_id}-node*"           # name of the SecurityGroup to be used with Nodes\n  #  instanceProfile: ""      # optional, if already set in controller args\n\n  userData: |\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary="BOUNDARY"\n\n    --BOUNDARY\n    Content-Type: text/x-shellscript; charset="us-ascii"\n\n    #!/bin/bash\n    echo "Running a custom user data script"\n    set -ex\n\n    IDX=1\n    DEVICES=$(lsblk -o NAME,TYPE -dsn | awk \'/disk/ {print $1}\')\n\n    for DEV in $DEVICES\n    do\n      mkfs.xfs /dev/$${DEV}\n      mkdir -p /local$${IDX}\n      echo /dev/$${DEV} /local$${IDX} xfs defaults,noatime 1 2 >> /etc/fstab\n      IDX=$(($${IDX} + 1))\n    done\n\n    mount -a\n\n    /usr/bin/chown -hR +999:+1000 /local*\n\n    --BOUNDARY--\n\n  tags:\n    InstanceType: "spark-compute-optimized"\n'))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Spark Jobs can use this provisioner to submit the jobs by adding ",(0,r.kt)("inlineCode",{parentName:"strong"},"tolerations")," to pod templates.")),(0,r.kt)("p",null,"e.g.,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tolerations:\n    - key: "spark-compute-optimized"\n      operator: "Exists"\n      effect: "NoSchedule"\n')),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Karpenter provisioner for memory optimized instances. This template uses the AWS Node template with Userdata.")),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Karpenter Provisioner - Memory Optimized Instances")),mdxType:"CollapsibleContent"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: spark-memory-optimized\n  namespace: karpenter\nspec:\n  kubeletConfiguration:\n    containerRuntime: containerd\n#    podsPerCore: 2\n#    maxPods: 20\n  requirements:\n    - key: "topology.kubernetes.io/zone"\n      operator: In\n      values: [${azs}b] #Update the correct region and zone\n    - key: "karpenter.sh/capacity-type"\n      operator: In\n      values: ["spot", "on-demand"]\n    - key: "node.kubernetes.io/instance-type" #If not included, all instance types are considered\n      operator: In\n      values: ["r5d.4xlarge","r5d.8xlarge","r5d.8xlarge"] # 2 NVMe disk\n    - key: "kubernetes.io/arch"\n      operator: In\n      values: ["amd64"]\n  limits:\n    resources:\n      cpu: 1000\n  providerRef: # optional, recommended to use instead of `provider`\n    name: spark-memory-optimized\n  labels:\n    type: karpenter\n    provisioner: spark-memory-optimized\n    NodeGroupType: SparkMemoryOptimized\n  taints:\n    - key: spark-memory-optimized\n      value: \'true\'\n      effect: NoSchedule\n  ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set\n\n---\napiVersion: karpenter.k8s.aws/v1alpha1\nkind: AWSNodeTemplate\nmetadata:\n  name: spark-memory-optimized\n  namespace: karpenter\nspec:\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 200Gi\n        volumeType: gp3\n        encrypted: true\n        deleteOnTermination: true\n  metadataOptions:\n    httpEndpoint: enabled\n    httpProtocolIPv6: disabled\n    httpPutResponseHopLimit: 2\n    httpTokens: required\n  subnetSelector:\n    Name: "${eks_cluster_id}-private*"        # Name of the Subnets to spin up the nodes\n  securityGroupSelector:                      # required, when not using launchTemplate\n    Name: "${eks_cluster_id}-node*"           # name of the SecurityGroup to be used with Nodes\n  instanceProfile: "${instance_profile}"      # optional, if already set in controller args\n  # RAID0 ARRAY config\n  userData: |\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary="BOUNDARY"\n\n    --BOUNDARY\n    Content-Type: text/x-shellscript; charset="us-ascii"\n\n    #!/bin/bash\n    echo "Running a custom user data script"\n    set -ex\n    yum install mdadm -y\n\n    DEVICES=$(lsblk -o NAME,TYPE -dsn | awk \'/disk/ {print $1}\')\n\n    DISK_ARRAY=()\n\n    for DEV in $DEVICES\n    do\n    DISK_ARRAY+=("/dev/$${DEV}")\n    done\n\n    if [ $${#DISK_ARRAY[@]} -gt 0 ]; then\n    mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${#DISK_ARRAY[@]} $${DISK_ARRAY[@]}\n    mkfs.xfs /dev/md0\n    mkdir -p /local1\n    echo /dev/md0 /local1 xfs defaults,noatime 1 2 >> /etc/fstab\n    mount -a\n    /usr/bin/chown -hR +999:+1000 /local1\n    fi\n\n    --BOUNDARY--\n\n  tags:\n    InstanceType: "spark-memory-optimized"    # optional, add tags for your own use\n\n'))),(0,r.kt)("p",null,"Spark Jobs can use this provisioner to submit the jobs by adding ",(0,r.kt)("inlineCode",{parentName:"p"},"tolerations")," to pod templates."),(0,r.kt)("p",null,"e.g.,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tolerations:\n    - key: "spark-memory-optimized"\n      operator: "Exists"\n      effect: "NoSchedule"\n')),(0,r.kt)(o.Z,{header:(0,r.kt)("h2",null,(0,r.kt)("span",null,"Deploying the Solution")),mdxType:"CollapsibleContent"},(0,r.kt)("p",null,"In this ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/emr-eks-karpenter"},"example"),", you will provision the following resources required to run Spark Jobs using EMR on EKS with ",(0,r.kt)("a",{parentName:"p",href:"https://karpenter.sh/"},"Karpenter")," as Autoscaler, as well as monitor job metrics using Amazon Managed Prometheus and Amazon Managed Grafana."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Creates EKS Cluster Control plane with public endpoint (recommended for demo/poc environment)"),(0,r.kt)("li",{parentName:"ul"},"One managed node group",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Core Node group with 2 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc."))),(0,r.kt)("li",{parentName:"ul"},"Enables EMR on EKS and creates two Data teams (",(0,r.kt)("inlineCode",{parentName:"li"},"emr-data-team-a"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"emr-data-team-b"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Creates new namespace for each team"),(0,r.kt)("li",{parentName:"ul"},"Creates Kubernetes role and role binding(",(0,r.kt)("inlineCode",{parentName:"li"},"emr-containers")," user) for the above namespace"),(0,r.kt)("li",{parentName:"ul"},"New IAM role for the team execution role"),(0,r.kt)("li",{parentName:"ul"},"Update ",(0,r.kt)("inlineCode",{parentName:"li"},"AWS_AUTH")," config map with ",(0,r.kt)("inlineCode",{parentName:"li"},"emr-containers")," user and ",(0,r.kt)("inlineCode",{parentName:"li"},"AWSServiceRoleForAmazonEMRContainers")," role"),(0,r.kt)("li",{parentName:"ul"},"Create a trust relationship between the job execution role and the identity of the EMR managed service account"),(0,r.kt)("li",{parentName:"ul"},"EMR Virtual Cluster for ",(0,r.kt)("inlineCode",{parentName:"li"},"emr-data-team-a")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"emr-data-team-b")))),(0,r.kt)("li",{parentName:"ul"},"Amazon Managed Prometheus workspace to remotely write metrics from Prometheus server"),(0,r.kt)("li",{parentName:"ul"},"Deploys the following Kubernetes Add-ons",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Managed Add-ons",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver"))),(0,r.kt)("li",{parentName:"ul"},"Self Managed Add-ons",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Karpetner, Apache YuniKorn(optional), FSx for Lustre(Optional), Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, VPA for Prometheus, AWS for FluentBit, CloudWatchMetrics for EKS")))))),(0,r.kt)("h3",{id:"prerequisites"},"Prerequisites:"),(0,r.kt)("p",null,"Ensure that you have installed the following tools on your machine."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"},"aws cli")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://Kubernetes.io/docs/tasks/tools/"},"kubectl")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://learn.hashicorp.com/tutorials/terraform/install-cli"},"terraform"))),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this ",(0,r.kt)("a",{parentName:"em",href:"https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html"},"userguide")," for supported regions.")),(0,r.kt)("h3",{id:"deploy"},"Deploy"),(0,r.kt)("p",null,"Clone the repository"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/awslabs/data-on-eks.git\n")),(0,r.kt)("p",null,"Navigate into one of the example directories and run ",(0,r.kt)("inlineCode",{parentName:"p"},"terraform init")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/analytics/terraform/emr-eks-karpenter\nterraform init\n")),(0,r.kt)("p",null,"Set AWS_REGION and Run Terraform plan to verify the resources created by this execution."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'export AWS_REGION="us-west-2"\nterraform plan\n')),(0,r.kt)("p",null,"This command may take between 20 and 30 minutes to create all the resources."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"terraform apply\n")),(0,r.kt)("p",null,"Enter ",(0,r.kt)("inlineCode",{parentName:"p"},"yes")," to apply."),(0,r.kt)("h3",{id:"verify-the-resources"},"Verify the resources"),(0,r.kt)("p",null,"Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-cluster --name emr-eks-karpenter\n\naws amp list-workspaces --alias amp-ws-emr-eks-karpenter\n")),(0,r.kt)("p",null,"Verify EMR on EKS Namespaces ",(0,r.kt)("inlineCode",{parentName:"p"},"emr-data-team-a")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"emr-data-team-b")," and Pod status for ",(0,r.kt)("inlineCode",{parentName:"p"},"Prometheus"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"Vertical Pod Autoscaler"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"Metrics Server")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Cluster Autoscaler"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks --region us-west-2 update-kubeconfig --name emr-eks-karpenter # Creates k8s config file to authenticate with EKS Cluster\n\nkubectl get nodes # Output shows the EKS Managed Node group nodes\n\nkubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams\n\nkubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods\n\nkubectl get pods --namespace=vpa  # Output shows Vertical Pod Autoscaler pods\n\nkubectl get pods --namespace=kube-system | grep  metrics-server # Output shows Metric Server pod\n\nkubectl get pods --namespace=kube-system | grep  cluster-autoscaler # Output shows Cluster Autoscaler pod\n"))),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Execute Spark job - NVMe SSD - Karpenter Compute Optimized Instances")),mdxType:"CollapsibleContent"},(0,r.kt)("h3",{id:"execute-the-sample-pyspark-job-to-trigger-compute-optimized-karpenter-provisioner"},"Execute the sample PySpark Job to trigger compute optimized Karpenter provisioner"),(0,r.kt)("p",null,"The following script requires four input parameters ",(0,r.kt)("inlineCode",{parentName:"p"},"virtual_cluster_id"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"job_execution_role_arn"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"cloudwatch_log_group_name")," & S3 Bucket to store PySpark scripts, Pod templates and Input data. You can get these values ",(0,r.kt)("inlineCode",{parentName:"p"},"terraform apply")," output values or by running ",(0,r.kt)("inlineCode",{parentName:"p"},"terraform output"),". For ",(0,r.kt)("inlineCode",{parentName:"p"},"S3_BUCKET"),", Either create a new S3 bucket or use an existing S3 bucket."),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-compute-provisioner/\n./execute_emr_eks_job.sh\nEnter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3\nEnter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a\nEnter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a\nEnter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://<bucket-name>: s3://example-bucket\n")),(0,r.kt)("p",null,"Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"),(0,r.kt)("h4",{id:"verify-the-job-execution"},"Verify the job execution"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods --namespace=emr-data-team-a -w\n"))),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Execute Spark job - EBS PVC - Karpenter Compute Optimized Instances")),mdxType:"CollapsibleContent"},(0,r.kt)("h3",{id:"execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-provisioner"},"Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter provisioner"),(0,r.kt)("p",null,"This pattern uses EBS volumes for data processing and compute optimized instances."),(0,r.kt)("p",null,"We will create Storageclass that will be used by drivers and executors. We'll create static Persistent Volume Claim (PVC) for the driver pod but we'll use dynamically created ebs volumes for executors."),(0,r.kt)("p",null,"Create StorageClass and PVC using example provided"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f emr-eks-karpenter-ebs.yaml\n")),(0,r.kt)("p",null,"Let's run the job"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cd analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs\n./execute_emr_eks_job.sh\n\n")),(0,r.kt)("p",null,"You'll notice the PVC ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-driver-pvc")," will be used by driver pod but Spark will create multiple ebs volumes for executors mapped to Storageclass ",(0,r.kt)("inlineCode",{parentName:"p"},"emr-eks-karpenter-ebs-sc"),". All dynamically created ebs volumes will be deleted once the job completes")),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Execute Spark job - NVMe SSD - Karpenter Memory Optimized Instances")),mdxType:"CollapsibleContent"},(0,r.kt)("h3",{id:"execute-the-sample-pyspark-job-to-trigger-memory-optimized-karpenter-provisioner"},"Execute the sample PySpark Job to trigger Memory optimized Karpenter provisioner"),(0,r.kt)("p",null,"This pattern uses the Karpenter provisioner for memory optimized instances. This template leverages the Karpenter AWS Node template with Userdata."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-memory-provisioner\n\n./execute_emr_eks_job.sh\n\n")),(0,r.kt)("p",null,"Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"),(0,r.kt)("h4",{id:"verify-the-job-execution-1"},"Verify the job execution"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods --namespace=emr-data-team-a -w\n"))),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Execute Spark job - NVMe SSD - Karpenter Graviton Instances")),mdxType:"CollapsibleContent"},(0,r.kt)("h3",{id:"execute-the-sample-pyspark-job-to-trigger-graviton-memory-optimized-karpenter-provisioner"},"Execute the sample PySpark Job to trigger Graviton Memory optimized Karpenter provisioner"),(0,r.kt)("p",null,"This pattern uses the Karpenter provisioner for Graviton memory optimized instances. This template leverages the Karpenter AWS Node template with Userdata."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-graviton-memory-provisioner\n\n./execute_emr_eks_job.sh\n\n")),(0,r.kt)("p",null,"Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"),(0,r.kt)("h4",{id:"verify-the-job-execution-2"},"Verify the job execution"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods --namespace=emr-data-team-a -w\n"))),(0,r.kt)("h2",{id:"fsx-for-lustre"},"FSx for Lustre"),(0,r.kt)("p",null,"Amazon FSx for Lustre is a fully managed shared storage option built on the world\u2019s most popular high-performance file system. It offers highly scalable, cost-effective storage, which provides sub-millisecond latencies, millions of IOPS, and throughput of hundreds of gigabytes per second. Its popular use cases include high-performance computing (HPC), financial modeling, video rendering, and machine learning. FSx for Lustre supports two types of deployments:"),(0,r.kt)("p",null,"For storage, EMR on EKS supports node ephemeral storage using hostPath where the storage is attached to individual nodes, and Amazon Elastic Block Store (Amazon EBS) volume per executor/driver pod using dynamic Persistent Volume Claims. However, some Spark users are looking for an HDFS-like shared file system to handle specific workloads like time-sensitive applications or streaming analytics."),(0,r.kt)("p",null,"In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage for running Spark jobs with EMR on EKS."),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Execute Spark job - FSx for Lustre Static Provisioning")),mdxType:"CollapsibleContent"},(0,r.kt)("p",null,"Fsx for Lustre Terraform module is disabled by default. Follow the steps to deploy the FSx for Lustre module and execute the Spark job."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Update the ",(0,r.kt)("inlineCode",{parentName:"li"},"analytics/terraform/emr-eks-karpenter/variables.tf")," file with the following")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'variable "enable_fsx_for_lustre" {\n  default     = true\n  description = "Deploys fsx for lustre addon, storage class and static FSx for Lustre filesystem for EMR"\n  type        = bool\n}\n\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Execute ",(0,r.kt)("inlineCode",{parentName:"li"},"terrafrom apply")," again. This will deploy FSx for Lustre add-on and all the necessary reosurces.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},"terraform apply -auto-approve\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Execute Spark Job by using ",(0,r.kt)("inlineCode",{parentName:"li"},"FSx for Lustre")," as a Shuffle storage for Driver and Executor pods with statically provisioned volume.\nExecute the Spark job using the below shell script.")),(0,r.kt)("p",null,"This script requires input parameters which can be extracted from ",(0,r.kt)("inlineCode",{parentName:"p"},"terraform apply")," output values."),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-static-pvc-shuffle-storage\n\n./fsx-static-spark.sh\n")),(0,r.kt)("p",null,"Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Verify the job execution events")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods --namespace=emr-data-team-a -w\n")),(0,r.kt)("p",null,"This will show the mounted ",(0,r.kt)("inlineCode",{parentName:"p"},"/data")," directory with FSx DNS name"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h\n\nkubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /static\n"))),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Execute Spark job - FSx for Lustre Dynamic Provisioning")),mdxType:"CollapsibleContent"},"Fsx for Lustre Terraform module is disabled by default. Follow the steps to deploy the FSx for Lustre module and execute the Spark job.",(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Update the ",(0,r.kt)("inlineCode",{parentName:"li"},"analytics/terraform/emr-eks-karpenter/variables.tf")," file with the following")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'variable "enable_fsx_for_lustre" {\n  default     = true\n  description = "Deploys fsx for lustre addon, storage class and static FSx for Lustre filesystem for EMR"\n  type        = bool\n}\n\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Execute ",(0,r.kt)("inlineCode",{parentName:"li"},"terrafrom apply")," again. This will deploy FSx for Lustre add-on and all the necessary reosurces.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},"terraform apply -auto-approve\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Execute Spark Job by using ",(0,r.kt)("inlineCode",{parentName:"li"},"FSx for Lustre")," as a Shuffle storage for Driver and Executor pods with dynamically provisioned FSx filesystem and Persistent volume.\nExecute the Spark job using the below shell script.")),(0,r.kt)("p",null,"This script requires input parameters which can be extracted from ",(0,r.kt)("inlineCode",{parentName:"p"},"terraform apply")," output values."),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-dynamic-pvc-shuffle-storage\n\n./fsx-static-spark.sh\n")),(0,r.kt)("p",null,"Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Verify the job execution events")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods --namespace=emr-data-team-a -w\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl exec -ti ny-taxi-trip-dyanmic-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h\n\nkubectl exec -ti ny-taxi-trip-dyanmic-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /dyanmic\n"))),(0,r.kt)("h2",{id:"apache-yunikorn---batch-scheduler"},"Apache YuniKorn - Batch Scheduler"),(0,r.kt)("p",null,"Apache YuniKorn is an open-source, universal resource scheduler for managing distributed big data processing workloads such as Spark, Flink, and Storm. It is designed to efficiently manage resources across multiple tenants in a shared, multi-tenant cluster environment.\nSome of the key features of Apache YuniKorn include:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Flexibility"),": YuniKorn provides a flexible and scalable architecture that can handle a wide variety of workloads, from long-running services to batch jobs."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Dynamic Resource Allocation"),": YuniKorn uses a dynamic resource allocation mechanism to allocate resources to workloads on an as-needed basis, which helps to minimize resource wastage and improve overall cluster utilization."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Priority-based Scheduling"),": YuniKorn supports priority-based scheduling, which allows users to assign different levels of priority to their workloads based on business requirements."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Multi-tenancy"),": YuniKorn supports multi-tenancy, which enables multiple users to share the same cluster while ensuring resource isolation and fairness."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Pluggable Architecture"),": YuniKorn has a pluggable architecture that allows users to extend its functionality with custom scheduling policies and pluggable components.")),(0,r.kt)("p",null,"Apache YuniKorn is a powerful and versatile resource scheduler that can help organizations efficiently manage their big data workloads while ensuring high resource utilization and workload performance."),(0,r.kt)("h2",{id:"architecture"},"Architecture"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache YuniKorn",src:n(8479).Z,width:"3060",height:"1683"})),(0,r.kt)(o.Z,{header:(0,r.kt)("h3",null,(0,r.kt)("span",null,"Apache YuniKorn Gang Scheduling with Karpenter")),mdxType:"CollapsibleContent"},"### Apache YuniKorn Gang Scheduling with Karpenter",(0,r.kt)("p",null,"Apache YuniKorn Scheduler add-on is disabled by default. Follow the steps to deploy the Apache YuniKorn add-on and execute the Spark job."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Update the ",(0,r.kt)("inlineCode",{parentName:"li"},"analytics/terraform/emr-eks-karpenter/variables.tf")," file with the following")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'variable "enable_yunikorn" {\n  default     = true\n  description = "Enable Apache YuniKorn Scheduler"\n  type        = bool\n}\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Execute ",(0,r.kt)("inlineCode",{parentName:"li"},"terrafrom apply")," again. This will deploy FSx for Lustre add-on and all the necessary reosurces.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},"terraform apply -auto-approve\n")),(0,r.kt)("p",null,"This example demonstrates the ",(0,r.kt)("a",{parentName:"p",href:"https://yunikorn.apache.org/docs/user_guide/gang_scheduling/"},"Apache YuniKorn Gang Scheduling")," with Karpenter Autoscaler."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-yunikorn-gangscheduling\n\n./execute_emr_eks_job.sh\n")),(0,r.kt)("h4",{id:"verify-the-job-execution-3"},"Verify the job execution"),(0,r.kt)("p",null,"Apache YuniKorn Gang Scheduling will create pause pods for total number of executors requested."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods --namespace=emr-data-team-a -w\n")),(0,r.kt)("p",null,"Verify the driver and executor pods prefix with ",(0,r.kt)("inlineCode",{parentName:"p"},"tg-")," indicates the pause pods.\nThese pods will be replaced with the actual Spark Driver and Executor pods once the Nodes are scaled and ready by the Karpenter."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"img.png",src:n(5845).Z,width:"2962",height:"1932"}))),(0,r.kt)(o.Z,{header:(0,r.kt)("h2",null,(0,r.kt)("span",null,"Cleanup")),mdxType:"CollapsibleContent"},"## Cleanup",(0,r.kt)("p",null,"This script will cleanup the environment using ",(0,r.kt)("inlineCode",{parentName:"p"},"-target")," option to ensure all the resources are deleted in correct order."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd analytics/terraform/emr-eks-karpenter/ && chmod +x cleanup.sh\n./cleanup.sh\n"))),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment")))}d.isMDXComponent=!0},5845:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/karpenter-yunikorn-gang-schedule-358958a77e7a238ef9edea38f09a7b9d.png"},8479:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/yunikorn-56ae296071d7f89d5e0de755c1d026ca.png"}}]);